{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/cl_intro_ws2025/blob/main/tutorials/Tutorial3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GszGq3W2gEWm"
      },
      "source": [
        "# Tutorial 3: PyTorch, Tensors and Word Embeddings\n",
        "This is the third tutorial with practical exercises for the lecture Introduction to Computational Linguistics in the winter semester 2025. Hands-on exercises are marked with ðŸ‘‹ âš’ and questions are marked with â“. Remember to first **store this notebook** in your Drive or GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Lesson 1: PyTorch Basics**\n",
        "\n",
        "In order to get started with deep learning and learn how to write a code for neural networks from scratch, we need to familiarize ourselves with the packages that can be used to this end. There are two basic open source machine learning frameworks that can be used to this end:\n",
        "\n",
        "*   TensorFlow (Google)\n",
        "*   Torch (Facebook, Google DeepMind, Twitter)\n",
        "\n",
        "Since these are high level core libraries, it is easier to use a framework that builds on top of it and adds some usability and documentation. We are going to work with [PyTorch](https://pytorch.org/). This first part of today's tutorial will introduce you to some core concepts of PyTorch before we start working with embeddings."
      ],
      "metadata": {
        "id": "isNE5fSKZ5Sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's first install pytorch\n",
        "!pip install torch torchvision"
      ],
      "metadata": {
        "id": "KQGsZ9icaDqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most basic and important concept in PyTorch is that of a **Tensor**. [What is a tensor?](https://en.wikipedia.org/wiki/Tensor). To speed up computation and offer more flexibility, PyTorch replaces numpy arrays with tensors."
      ],
      "metadata": {
        "id": "O5VDj_8-aGjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy\n",
        "\n",
        "# Seed for random number generator to ensure reproducibility of\n",
        "# random initializations\n",
        "torch.manual_seed(1)\n",
        "\n",
        "#Difference between tensor and numpy array\n",
        "a = torch.ones(5)\n",
        "print(\"Tensor: \")\n",
        "print(a)\n",
        "print(\"Numpy array: \")\n",
        "print(a.numpy(), \"\\n\")\n",
        "\n",
        "\n",
        "# This creates a randomly initialized 5 x 3 matrix\n",
        "rand = torch.rand(5,3)\n",
        "print(\"Randomly initialized tensor: \")\n",
        "print(rand)\n",
        "print(\"The shape of the matrix is its dimensions (here 5x3): \", rand.shape, \"\\n\")\n",
        "\n",
        "# This creates a 5 x 3 matrix filled with zeros and of dtype long\n",
        "# There are eight datatypes in tensor, this one is a datatype of 64-bit integer (signed)\n",
        "# Here are the others: https://pytorch.org/docs/stable/tensors.html\n",
        "zeros = torch.zeros(4,3, dtype=torch.long)\n",
        "print(\"Tensor initialized with zeros: \", zeros, \"\\n\")\n",
        "\n",
        "# Directly initialize a tensor with data\n",
        "data = torch.tensor([[5.5, 3], [1, 2]])\n",
        "print(\"Tensor initilialized with data: \", data, \"\\n\")\n",
        "\n",
        "# You can redefine an existing tensor\n",
        "redefined = rand.new_ones(5, 3, dtype=torch.double)\n",
        "print(\"Redefined randomly initialized tensor: \")\n",
        "print(redefined)\n",
        "\n",
        "redefined_too = torch.rand_like(redefined, dtype=torch.float)\n",
        "print(\"\\nInitializing randomly based on the size of redefined: \")\n",
        "print(redefined_too)\n",
        "\n",
        "\n",
        "# Get the size - this is actually a tuple that supports tuple operations\n",
        "print(redefined_too.size())\n",
        "#or\n",
        "print(redefined.shape)"
      ],
      "metadata": {
        "id": "aPXpCtEfaHI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors in torch also support basic operations:"
      ],
      "metadata": {
        "id": "4rC7iO1kaQE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Addition of tensors matching in size\n",
        "x = torch.rand(5, 3)\n",
        "y= torch.rand(5, 3)\n",
        "print(\"Addition: \", x + y, \"\\n\")\n",
        "print(\"Addition alternative syntax: \", torch.add(x, y), \"\\n\")\n",
        "\n",
        "# Addition with providing a tensor as argument stores the result in \"result\"\n",
        "result = torch.empty(5, 3)\n",
        "torch.add(x, y, out=result)\n",
        "print(\"Addition with tensor as argument: \", result, \"\\n\")"
      ],
      "metadata": {
        "id": "GTYwFHjuaQbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing operations of tensors follow the numpy standard:"
      ],
      "metadata": {
        "id": "pUJCc9uOaY8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Indexing\n",
        "print(\"X: \", x, \"\\n\")\n",
        "print(\"Element at index one of each row of the matrix \", x[:, 1], \"\\n\")"
      ],
      "metadata": {
        "id": "UJE4a2f2aZbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resizing: if you wish to change the shape of the tensor you can use torch.view:"
      ],
      "metadata": {
        "id": "oh5WmfhiabRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise: resizing: what effect do the following\n",
        "# resize operations have on the tensors\n",
        "x = torch.randn(4, 4)\n",
        "y = x.view(16)\n",
        "z = x.view(-1, 8)\n",
        "print(\"Resizing: \")\n",
        "print(\"Original\")\n",
        "print(x)\n",
        "print(\"Resized view(16)\", y, \"\\n\")\n",
        "print(\"Resized view(-1, 8)\", z, \"\\n\")"
      ],
      "metadata": {
        "id": "CuJqctfCadDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "One of the reasons why we use **tensors** is *vectorized operations*: operations that be conducted in parallel over a particular dimension of a tensor."
      ],
      "metadata": {
        "id": "ASZL94u_liwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.arange(1, 36, dtype=torch.float32).reshape(5, 7)\n",
        "print(\"Data is:\", data)\n",
        "\n",
        "# We can perform operations like *sum* over each row...\n",
        "print(\"Taking the sum over rows:\")\n",
        "print(data.sum(dim=1)) #(5,)\n",
        "\n",
        "# or over each column.\n",
        "print(\"Taking the sum over columns:\")\n",
        "print(data.sum(dim=0)) #(7,)\n",
        "\n",
        "# Other operations are available:\n",
        "print(\"Taking the standard deviation over rows:\")\n",
        "print(data.std(dim=1))"
      ],
      "metadata": {
        "id": "VmuIEbsjlopj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ Write code that creates a `torch.tensor` with the following contents:\n",
        "$\\begin{bmatrix} 1 & 2.2 & 9.6 \\\\ 4 & -7.2 & 6.3 \\end{bmatrix}$\n",
        "\n",
        "Now compute the average of each row (`.mean()`) and each column.\n",
        "\n",
        "What's the shape of the results?"
      ],
      "metadata": {
        "id": "BJYk8Xx8lzY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "pDdyxJROnKJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnC6HAtqgHGK"
      },
      "source": [
        "---\n",
        "\n",
        "## **Lesson 2: Word Embeddings**\n",
        "\n",
        "A vector representation of words trained with a neural network is called a word embedding. One of the most popular methods for training embeddings is called word2vec, which is a method for training embeddings from large natural language corpora.\n",
        "\n",
        "\n",
        "`word2vec` literature:\n",
        "  - Mikolov, T.,  Chen, K., Corrado, G., & Dean, J. (2013). [Efficient estimation of word representations in vector space](https://arxiv.org/abs/1301.3781). Corr abs/1301.3781.\n",
        "  - Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). [Distributed representations of words and phrases and their compositionally](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). *Advances in neural information processing systems*. 2013.\n",
        "\n",
        "\n",
        "- Other variants of embeddings training:\n",
        "  - `fasttext` from Facebook\n",
        "  - `GloVe` from Stanford NLP Group\n",
        "- There are many ways to train word embeddings:\n",
        "  - `gensim`: Simplest and straightforward implementation of `word2vec` and `GloVe`\n",
        "  - Training based on deep learning packages (e.g., `keras`, `tensorflow`)\n",
        "  - `spacy` (It comes with a pre-trained embeddings models, using GloVe.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfcYOrYDmpfv"
      },
      "source": [
        "When multiplying a matrix on the hidden layer with the one-hot encoding, we obtain one row of the matrix. So the matrix serves as a lookup table for embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRS9y_W1pv_C"
      },
      "source": [
        "### Using Pre-Trained Embeddings\n",
        "\n",
        "The code below exemplifies how to load a trained embedding model in the gensim library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K0JS2yIf9ey"
      },
      "outputs": [],
      "source": [
        "# Let's first load a small subset of word2vec embeddings that have been trained on a\n",
        "# large corpus of news documents\n",
        "!wget https://github.com/dgromann/cl_intro_ws2025/raw/main/word2vec_embeddings.bin\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT3equb6aKNy"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Let's load the model\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(\"word2vec_embeddings.bin\", binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the length fo the whole vocabulary\n",
        "print(\"Length of the vocabulary\",len(model.key_to_index))\n",
        "\n",
        "# Print the embedding of a specific word\n",
        "print(\"Embedding for the word good: \", model[\"good\"])"
      ],
      "metadata": {
        "id": "D_Rm7ukcl0Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ðŸ‘‹ âš’ How many dimensions (numbers) does each vector in this trained embedding model have? Try to find this out with code, not by counting."
      ],
      "metadata": {
        "id": "8Nb8E4AYl6SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code goes here"
      ],
      "metadata": {
        "id": "zJdLUqzamYIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use embeddings to evaluate how similar two words are.\n",
        "\n",
        "ðŸ‘‹ âš’ How can we get the first most similiar word of good from the list of the top 5 most similar words? In other words, how can you return and print only the word and not the tuple of word and cosine seimilarity?"
      ],
      "metadata": {
        "id": "z0wPsvgHl3wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.most_similar('good'))\n",
        "\n",
        "# Get the top 5 most similar words of \"good\"\n",
        "most_similar = model.most_similar(\"good\", topn=5)\n",
        "print(most_similar)\n",
        "\n",
        "# Your code to get the first word from that list of top 5"
      ],
      "metadata": {
        "id": "hMaZSp_Wl34h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use these embeddings to obtain similar words in other pairs with the analogy task a is to b as c is to d, e.g. *man is to woman as king is to ?*"
      ],
      "metadata": {
        "id": "y4jr1MyxnaKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether our embeddings are good at the analogy task\n",
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "3yfGt0BDl4AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ Complete the following function `analogy` that takes three words in the analogy as input and should return the correct fourth result. For instance, good is to best as bad ist to?\n",
        "\n",
        "So `analogy(\"good\", \"best\", \"bad\")` should return the word ``\"worst\"``."
      ],
      "metadata": {
        "id": "rGaa-MRunjUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analogy(a, b, c):\n",
        "  #Your code goes here\n",
        "\n",
        "\n",
        "print(analogy(\"France\", \"Paris\", \"Austria\"))\n",
        "print(analogy(\"good\", \"best\", \"bad\"))"
      ],
      "metadata": {
        "id": "FINqLoLbnlaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use matplotlib to visualize the proximity of words in vector space."
      ],
      "metadata": {
        "id": "cntAtMnkugSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_pca_scatterplot(model, words):\n",
        "\n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)"
      ],
      "metadata": {
        "id": "k5VAQte-pzGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_pca_scatterplot(model,\n",
        "                        ['coffee', 'tea', 'beer', 'wine', 'water',\n",
        "                         'hamburger', 'pizza',  'sushi', 'meatballs',\n",
        "                         'dog', 'horse', 'cat', 'monkey', 'parrot', 'lizard',\n",
        "                         'France', 'Germany', 'Hungary',\n",
        "                         'school', 'college', 'university', 'institute'])"
      ],
      "metadata": {
        "id": "fZGXtTzGqG3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Lesson 3: Language Model and Embeddings in PyTorch**\n",
        "\n",
        "\n",
        "In PyTorch, it is also possible to create your own neural network."
      ],
      "metadata": {
        "id": "T444Bz0pbXoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Again random number generator to ensure reproducibility\n",
        "torch.manual_seed(1)"
      ],
      "metadata": {
        "id": "ClSOkeTscsHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use nn.Linear(H_in, H_out) to create a a linear layer. This will take a matrix of (N, *, H_in) dimensions and output a matrix of (N, *, H_out). The * denotes that there could be arbitrary number of dimensions in between. The linear layer performs the operation Ax+b, where A and b are initialized randomly. If we don't want the linear layer to learn the bias parameters, we can initialize our layer with bias=False.\n"
      ],
      "metadata": {
        "id": "o9yM3_vLdPR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the inputs\n",
        "input = torch.ones(2,3,4)\n",
        "print(\"Input \", input)\n",
        "\n",
        "# N* H_in -> N*H_out\n",
        "linear = nn.Linear(4, 2)\n",
        "linear_output = linear(input)\n",
        "linear_output\n",
        "\n",
        "\n",
        "list(linear.parameters()) # Ax + b"
      ],
      "metadata": {
        "id": "LPlDrEGcdYcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add an activation function:"
      ],
      "metadata": {
        "id": "fdrEYgp8da57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid = nn.Sigmoid()\n",
        "output = sigmoid(linear_output)\n",
        "output"
      ],
      "metadata": {
        "id": "BtU6mphtdeZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of creating intermediate layers and passing variables around, we can create a sequence:"
      ],
      "metadata": {
        "id": "VBZ7ndm7dkJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block = nn.Sequential(\n",
        "    nn.Linear(4, 2),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "input = torch.ones(2,3,4)\n",
        "output = block(input)\n",
        "output"
      ],
      "metadata": {
        "id": "s46z9vK0dmT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Word Embeddings with PyTorch\n",
        "\n",
        "As a toy example, we will convert words to word embeddings. The preprocessing below should be done more elegantly and not as simply as below.\n",
        "\n",
        "The below implementation is just a toy implementation. For a better version, see [this word2vec implementation in Pytorch](https://adoni.github.io/2017/11/08/word2vec-pytorch/)"
      ],
      "metadata": {
        "id": "sQpFcLFmdtRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# We will use Shakespeare Sonnet 2\n",
        "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\"\n",
        "\n",
        "training_sentence = test_sentence.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
        "print(training_sentence)"
      ],
      "metadata": {
        "id": "uXXz7bTIdvjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next let's find our vocabulary, i.e., all the unique words in the training data:"
      ],
      "metadata": {
        "id": "k7bSliLaeTJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = set(w for w in training_sentence)\n",
        "vocabulary"
      ],
      "metadata": {
        "id": "S1dg1EbKeXT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We introduce a special token, `<unk>`, to tackle the words that are out of vocabulary. We could pick another string for our unknown token if we wanted. The only requirement here is that our token should be unique: we should only be using this token for unknown words. We will also add this special token to our vocabulary."
      ],
      "metadata": {
        "id": "ZUOay-e6j54v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary.add(\"<unk>\")"
      ],
      "metadata": {
        "id": "N8pKdgzxj6lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will create the index for our vocabulary - one index to word and one word to index to make looking up words easier:"
      ],
      "metadata": {
        "id": "zNKMYIrpgQek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ix_to_word = sorted(list(vocabulary))\n",
        "word_to_ix = {word: ind for ind, word in enumerate(ix_to_word)}"
      ],
      "metadata": {
        "id": "xdkT9sS6gSLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ How can we now lookup which word is the fifth word in our index list?"
      ],
      "metadata": {
        "id": "GTNTeYu6g19y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "TBR4hrx2g_bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ How can we lookup the index for \"within\"?"
      ],
      "metadata": {
        "id": "7ncxl2_7kAyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "mUygMpAXkEdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a very simple solution of building trigrams to train our model."
      ],
      "metadata": {
        "id": "eAmYSNhWhK_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigrams = [([training_sentence[i], training_sentence[i + 1]], training_sentence[i + 2])\n",
        "            for i in range(len(training_sentence)-2)]\n",
        "print(trigrams)"
      ],
      "metadata": {
        "id": "5ss1E4wshLZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following class defines the exact setup of our embedding model."
      ],
      "metadata": {
        "id": "qmm9Y1FZhU93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NGramLanguageModeler(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, window_size, hidden_dim):\n",
        "        super(NGramLanguageModeler, self).__init__()\n",
        "\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.hidden_layer = nn.Sequential(\n",
        "            nn.Linear(window_size * embedding_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        layer1 = self.hidden_layer(embeds)\n",
        "        output = self.output_layer(layer1)\n",
        "        probabilities = nn.functional.log_softmax(output, dim=1)\n",
        "        return probabilities"
      ],
      "metadata": {
        "id": "JGAwF7xmhVLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ Now let's train the model. Try to adapt the hyperparamters so that the total_loss is reduced. These include in the initial settings:\n",
        "\n",
        "*   Dimensionality of the embeddings: 10\n",
        "*   Dimensionality of the hidden layer: 128\n",
        "*   Number of epochs: 10\n",
        "*   Learning rate: 0.002\n",
        "*   Loss function (`NLLLoss()` Negative Log Likelihood right now)\n",
        "\n",
        "The window size can in this toy example not be changed, since we train on trigrams (so we only have two context words in the set).\n",
        "\n",
        "The code cell before, the `class NGramLanguageModeler(nn.Module)` defines the exact setup of the network. For the setup one thing to change could be the  activation functions (ReLU and Log-Softmax right now) - others can be found [here](https://pytorch.org/docs/stable/nn.html) for sequential layers and here for other uses as [nn.functional](https://pytorch.org/docs/stable/nn.functional.html)).\n",
        "\n",
        "Just for comparison, this is the initial output with the first settings of the notebook:\n",
        "\n",
        "`See how loss decreases with each epoch:  [4.502952638980561, 4.453238930322428, 4.404419487556525, 4.356399970771992, 4.309086953644204, 4.262444785210938, 4.216389432417608, 4.170906921403598, 4.125928629816106, 4.081349758975274]\n",
        "Loss of the last epoch: 4.081349758975274`"
      ],
      "metadata": {
        "id": "B1oR94tehjJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 2\n",
        "embedding_dim = 10\n",
        "hidden_dim = 128\n",
        "num_epochs = 10\n",
        "\n",
        "losses = []\n",
        "# Negative log likelihood\n",
        "loss_function = nn.NLLLoss()\n",
        "ngram_model = NGramLanguageModeler(len(vocabulary), embedding_dim, window_size, hidden_dim)\n",
        "\n",
        "# What do SGD and lr mean? What happenes if you change them?\n",
        "optimizer = torch.optim.SGD(ngram_model.parameters(), lr=0.002)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for context, target in trigrams:\n",
        "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
        "        # into integer indices and wrap them in tensors)\n",
        "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
        "\n",
        "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
        "        # new instance, you need to zero out the gradients from the old\n",
        "        # instance\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Step 3. Run the forward pass, getting probabilities over next\n",
        "        # words - the size of this tensor is 87 corresponding to the size of the vocabulary\n",
        "        probabilities = ngram_model.forward(context_idxs)\n",
        "\n",
        "        # Step 4. Compute your loss function. The target word (gold standard label) needs to be\n",
        "        # wrapped in a tensor.\n",
        "        loss = loss_function(probabilities, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "\n",
        "        # Step 5. Do the backward pass and update the gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "        total_loss += loss.item()\n",
        "    losses.append(total_loss / len(trigrams))\n",
        "\n",
        "print(\"See how loss decreases with each epoch: \", losses)\n",
        "print(\"Loss of the last epoch:\", losses[-1])"
      ],
      "metadata": {
        "id": "9RNonYGwh3wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ Implement the cosine simmilarity for the trained embeddings. For this you need to find the index of the two words you wish to compare, get their embeddings, convert them into numpy arrays, and run them through the cosine function provided."
      ],
      "metadata": {
        "id": "PmtkJ2mtjFOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# Tensor to look up specific embeddings\n",
        "lookup_tensor = torch.tensor(list(word_to_ix.values()), dtype=torch.long)\n",
        "\n",
        "# Embedding for the first word in the vocabulary\n",
        "lookup_embeds = ngram_model.embeddings(lookup_tensor)[0]\n",
        "print(lookup_embeds)\n",
        "\n",
        "#cosine = np.dot(A,B)/(norm(A)*norm(B))"
      ],
      "metadata": {
        "id": "ysRE02YkjFdK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXDb4voDssoIAouAmq7Mrg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}